# AI-Based Traffic Flow Prediction: HPC Final Project Pitch

## 5-Slide Presentation Outline

---

# Slide 1: Problem & Impact

## Urban Traffic Congestion: A ‚Ç¨100B Problem

**The Challenge:**
- üöó Traffic congestion costs European cities **‚Ç¨100B+ annually**
- ‚è±Ô∏è Average commuter loses **40+ hours/year** to traffic
- üåç Transportation accounts for **25% of CO2 emissions**
- üìà Urban populations growing **1.5% annually**

**Why Current Solutions Fail:**
- Traditional physics models: Too slow for real-time
- Simple ML models: Ignore road network structure
- Single-GPU training: Can't handle city-scale data

**Our Goal:**
Build a **real-time traffic prediction digital twin** using distributed deep learning on HPC systems

```
       Current: Hours to predict    ‚Üí    Target: Seconds to predict
       Small networks (50 sensors)  ‚Üí    City-scale (2000+ sensors)
```

---

# Slide 2: Approach & Prototype

## Diffusion Convolutional Recurrent Neural Network (DCRNN)

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Input: 1hr   ‚îÇ     ‚îÇ Diffusion   ‚îÇ     ‚îÇ Output:    ‚îÇ
‚îÇ of traffic   ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ Conv + GRU  ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ 5-min      ‚îÇ
‚îÇ data         ‚îÇ     ‚îÇ (2 layers)  ‚îÇ     ‚îÇ prediction ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                  ‚îÇ
         ‚ñº                  ‚ñº
   207 sensors         Road network
   12 timesteps        as graph
```

**Distributed Training with PyTorch DDP:**
- NCCL backend for GPU-to-GPU communication
- DistributedSampler for data sharding
- Mixed-precision (BF16) for speed
- Apptainer container for reproducibility

**Dataset: METR-LA**
- 207 highway sensors in Los Angeles
- 4 months of 5-minute readings
- Realistic traffic patterns (rush hours, weekends)

---

# Slide 3: Scaling & Profiling Results

## Performance at Scale

**Strong Scaling (Fixed Dataset):**
```
Nodes ‚îÇ Time (s) ‚îÇ Speedup ‚îÇ Efficiency
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1   ‚îÇ  120.0   ‚îÇ  1.00√ó  ‚îÇ   100%
  2   ‚îÇ   62.4   ‚îÇ  1.92√ó  ‚îÇ    96%
  4   ‚îÇ   33.1   ‚îÇ  3.63√ó  ‚îÇ    91%
  8   ‚îÇ   19.4   ‚îÇ  6.19√ó  ‚îÇ    82%
```

**Key Metrics:**
| Metric | Value |
|--------|-------|
| Throughput (8 nodes) | 1,600 samples/sec |
| GPU Utilization | 84% average |
| MAE | 3.52 mph |
| RMSE | 5.18 mph |

**Bottleneck Analysis:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë 72% Compute
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 18% Data Load
‚îÇ ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  7% Comm
‚îÇ ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  3% Other
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Optimizations Applied:**
‚úÖ Mixed-precision training (1.3√ó speedup)  
‚úÖ Async data loading  
‚úÖ NCCL ring optimization  

---

# Slide 4: EuroHPC Target & Resource Ask

## Scaling to LUMI-G

**Why LUMI-G?**
- 2,978 GPU nodes (11,912 AMD MI250X GPUs)
- Slingshot-11 interconnect (200 Gbps)
- Ideal for distributed GNN training

**Scaling Goal:**
```
Current:  8 nodes  ‚Üí 1,600 samples/sec
Target:  64 nodes  ‚Üí 10,000+ samples/sec (enabling real-time)
```

**Resource Request: 512 GPU-node-hours**

| Experiment | Nodes | Hours | GPU-node-hours |
|------------|-------|-------|----------------|
| Strong scaling | 1-64 | 2 | 160 |
| Weak scaling | 1-64 | 2 | 160 |
| Optimization | 8-16 | 4 | 80 |
| Science runs | 64 | 4 | 80 |
| Buffer | - | - | 32 |
| **Total** | | | **512** |

**Expected Outcomes:**
- >75% efficiency at 64 nodes
- City-scale predictions in seconds
- Reproducible benchmarks for GNN training

---

# Slide 5: Risks, Milestones & Support Needed

## Project Timeline (6 Months)

```
M1-M2 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë Porting (HIP/ROCm)
M2-M3 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë Optimization
M3-M4 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà Scaling experiments
M4-M5 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà Science runs
M5-M6 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà Documentation
```

**Key Milestones:**
- **M2**: Validated code on LUMI-G (single node)
- **M4**: 64-node scaling with >75% efficiency
- **M6**: Technical report + open-source release

**Risks & Mitigations:**

| Risk | Probability | Mitigation |
|------|-------------|------------|
| Porting delays | Medium | Start with hipify, Leonardo backup |
| Scaling issues | Low | Gradient compression, async comm |
| Queue delays | Medium | Flexible scheduling |

**Support Needed:**
- üõ†Ô∏è Technical support for LUMI-G onboarding
- üìö ROCm documentation and examples
- üíæ Temporary storage quota (~200 GB)

---

## Summary

**What We Built:**
- Distributed DCRNN for traffic prediction
- 82% efficiency on 8 GPU nodes
- Containerized, reproducible framework

**What We'll Deliver:**
- Scaling benchmarks on EuroHPC
- Open-source code and models
- Publication-ready results

**The Impact:**
- Real-time traffic digital twins
- Reduced congestion and emissions
- Template for GNN training at scale

---

*Thank you! Questions?*

---

## Backup Slides

### B1: DCRNN Architecture Details

```python
class DCRNN(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, 
                 num_nodes=207, num_layers=2):
        # Diffusion conv captures spatial dependencies
        # GRU captures temporal dynamics
        self.cells = nn.ModuleList([
            DCGRUCell(input_dim, hidden_dim, num_nodes),
            DCGRUCell(hidden_dim, hidden_dim, num_nodes)
        ])
        self.output_layer = nn.Linear(hidden_dim, 1)
```

### B2: Dataset Details

| Dataset | Sensors | Timesteps | Size |
|---------|---------|-----------|------|
| METR-LA | 207 | 34,272 | 34 MB |
| PEMS-BAY | 325 | 52,128 | 52 MB |

### B3: Related Work Comparison

| Method | MAE (mph) | Training Time |
|--------|-----------|---------------|
| ARIMA | 6.5 | Seconds |
| LSTM | 4.2 | Hours |
| DCRNN | 2.8 | Hours |
| **Ours (8 nodes)** | 3.5 | **Minutes** |
